{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51da1519",
   "metadata": {},
   "source": [
    "## Initialization (#init)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "The following is a script for replicating figures and tables presented in paper \"Reasoning about Regular Properties: A Comparative Study\"\n",
    "\n",
    "Run `Cell` -> `Run All` to generate files in `figs/` directory. See the `README` for more information about the context of the work.\n",
    "\n",
    "The first cell initializes the notebook and helpers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1471363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "import pandas\n",
    "import tabulate\n",
    "import pickle\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "import itertools\n",
    "from statistics import mean\n",
    "from collections import defaultdict\n",
    "from enum import Enum\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# Short hack for display of images in jupyter notebook\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>div.output_area pre {white-space: pre;}</style>\"))\n",
    "\n",
    "def is_real(x):\n",
    "    \"\"\"Determines, whether `x` corresponds to real value\"\"\"\n",
    "    try:\n",
    "        float(x)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False        \n",
    "\n",
    "def value_to_float(val, timeout):\n",
    "    \"\"\"Converts @p val to timeout or NaN\"\"\"\n",
    "    if val.startswith('TO') or val == 'ERR':\n",
    "        return timeout\n",
    "    else:\n",
    "        return np.NaN\n",
    "    \n",
    "def to_float_with_timeout(series, timeout):\n",
    "    \"\"\"Converts @p series to floats or timeouts\"\"\"\n",
    "    return [float(f) if is_real(f) else value_to_float(f, timeout) for f in series]\n",
    "\n",
    "def df_to_float(df, timeout, allow_timeouts=False):\n",
    "    \"\"\"Transforms @p df to dataframe with floats only, timeouts are either converted to NaN or to @p timeout\"\"\"\n",
    "    return df.apply(lambda x: to_float_with_timeout(x, timeout)).dropna(how='all', axis='columns')\n",
    "\n",
    "def unify_tool(tool):\n",
    "    \"\"\"Unifies the names of the tool\n",
    "    \n",
    "    This is due to the fact, that we tried different parameters in past and need to unify it in the data\n",
    "\n",
    "    Add calls to `tool.replace('str', '') to remove some alternations of tools\n",
    "    \"\"\"\n",
    "    return tool\n",
    "    #return tool_to_output[tool]\n",
    "        \n",
    "def get_benchmark_name(bench):\n",
    "    \"\"\"Returns classification for @p bench benchmark\"\"\"\n",
    "    if 'bool_comb/ere/QF_SLIA_Norn' in bench or 'bool_comb/ere/QF_S_sygus_qgen' in bench:\n",
    "        return 'b-smt'\n",
    "    elif 'email_filter' in bench:\n",
    "        return 'b-regex'\n",
    "    elif 'bool_comb/ere/boolean_and_loops' in bench or 'bool_comb/ere/date' in bench or 'bool_comb/ere/det_blowup' in bench or 'bool_comb/ere/password' in bench:\n",
    "        return 'b-hand-made'\n",
    "    elif 'armc-inclusion' in bench or \"automata_inclusion\" in bench:\n",
    "        return 'b-armc-incl'\n",
    "    elif 'bool_comb/cox' in bench or 'bool_comb/intersect' in bench:\n",
    "        return 'b-param'\n",
    "    else:\n",
    "        d = os.path.dirname(bench).split(os.sep)\n",
    "        return f\"? {'_'.join(d[:2])}\"\n",
    "\n",
    "# Directory, where figures are stored\n",
    "FIGS_DIR = \"figs\"\n",
    "if not os.path.exists(FIGS_DIR):\n",
    "    os.mkdir(FIGS_DIR)\n",
    "    \n",
    "def save_figure(fig, ext=\".png\"):\n",
    "    \"\"\"Stores @p fig at `figs/fig.@ext`\"\"\"\n",
    "    tgt = os.path.join(FIGS_DIR, fig + ext)    \n",
    "    print(f\"Saving to {tgt}\")\n",
    "    if ext == \".png\":\n",
    "        plt.savefig(tgt, backend=\"cairo\", bbox_inches=\"tight\", pad_inches=0.2)\n",
    "    else:\n",
    "        plt.savefig(tgt, bbox_inches=\"tight\", pad_inches=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcb2310",
   "metadata": {},
   "source": [
    "Two helper dictionaries that converst the name of the tools between data -> paper -> latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a13ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_to_latex = {\n",
    "    'Automata': '\\\\dotnet',\n",
    "    'Brics': '\\\\brics',\n",
    "    'mata': '\\\\mata',\n",
    "    'Mona': '\\mona',\n",
    "    'VATA': '\\\\vata',\n",
    "}\n",
    "tool_to_output = {\n",
    "    'automata': 'Automata',\n",
    "    'bricks': 'Brics',\n",
    "    'mata-nfa': 'mata',\n",
    "    'mona': 'Mona',\n",
    "    'vata': 'VATA',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40905a3",
   "metadata": {},
   "source": [
    "Helper scripts for conversion of source files to `pandas.DataFrame` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafb4d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pandas(src: str, timeout=60) -> pandas.DataFrame:\n",
    "    \"\"\"Converts @p src to pandas dataframe\"\"\"\n",
    "    df = pandas.read_csv(src, delimiter=';')\n",
    "    columns = [\n",
    "        col for col in df.columns \n",
    "        if (col == 'name' or not col.endswith('result')) # Add some other conditions\n",
    "    ]\n",
    "    df = df[columns]\n",
    "    df['name'] = [name for name in df['name']] # add name.replace('str', '') to remove parts of benchmarks\n",
    "    def rounder(x):\n",
    "        if x == 'TO' or (is_real(x) and float(x) > timeout):\n",
    "            return f'TO{timeout}'\n",
    "        else:\n",
    "            return x\n",
    "    df = df.map(rounder)\n",
    "    return df\n",
    "\n",
    "def df_to_plottable(df, timeout=60):\n",
    "    \"\"\"Converts dataframe to a format better for certain plots\"\"\"\n",
    "    columns = ['benchmark', 'file', 'tool', 'duration']\n",
    "    data = []\n",
    "    for index, row in df.iterrows():\n",
    "        file = row['name']\n",
    "        bench = get_benchmark_name(file)\n",
    "        for tool, duration in [r for r in row.items() if r[0] != 'name']:\n",
    "            if is_real(duration):\n",
    "                data.append([bench, file, unify_tool(tool), float(duration)])\n",
    "            elif duration == 'MISSING':\n",
    "                continue\n",
    "            else:\n",
    "                data.append([bench, file, unify_tool(tool), timeout])\n",
    "    return pandas.DataFrame(data, columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68843179",
   "metadata": {},
   "source": [
    "List of benchmarks. Each tripplet consists of (1) benchmark name, (2) timeout, (3) source path.\n",
    "\n",
    "Comment out benchmarks that you wish to exclude or include your own files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f7f565-f45b-41d1-b731-6d59933f02b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in [f for f in os.listdir('.') if f.endswith('.csv')]:\n",
    "    timeout = re.search(\"timeout-(\\w+)\", file).group(1)\n",
    "    tag = \"\".join(a.capitalize() for a in itertools.takewhile(lambda x: not x[0].isdigit(), file.split('-')))\n",
    "    print(f\"    ('{tag}', {timeout}, '{file}'), \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c178a631",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarks = [\n",
    "    ('BenchDoubleAutomataInclusion', 60, 'bench-double-automata-inclusion-2023-09-01-14-32-41-timeout-60-jobs-6.csv'), \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa22605",
   "metadata": {},
   "source": [
    "## Input dataframe (#dataframe)\n",
    "\n",
    "Creates three dataframes:\n",
    "  1. `dataframe_map` maps benchmark to its dataframe (created by `to_pandas()`)\n",
    "  2. `plottable_map_per_grp` maps benchmarks to its plottable dataframe (created by `df_to_plottable()`)\n",
    "  3. `overall_df` consists of all dataframes from `dataframe_map` merged into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bf416c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_map = {}\n",
    "plottable_map_per_grp = {}\n",
    "\n",
    "for bench, timeout, src in benchmarks:\n",
    "    key = f\"{bench}#{timeout}\"\n",
    "    df = to_pandas(src, timeout=timeout)\n",
    "    dataframe_map[key] = df\n",
    "    plottable_map_per_grp[key] = df_to_plottable(df, timeout=timeout)\n",
    "    \n",
    "overall_df = pandas.concat([df for key, df in dataframe_map.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d910670",
   "metadata": {},
   "source": [
    "Specification of color map further used in figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475e2e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = sorted([unify_tool(t) for t in overall_df.columns if t != 'name' and t != 'bench'])\n",
    "tool_len = len(tools)\n",
    "color_map = {\n",
    "    t: c for (t, c) in zip(tools, mpl.colormaps['tab20'].resampled(tool_len).colors)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bd5170",
   "metadata": {},
   "source": [
    "## Summary of statistics (#stats)\n",
    "\n",
    "For two sets of data (`afa` and `bre` benchmarks) creates (1) a HTML table, and (2) LaTeX table.\n",
    "Each table summaries the data for each benchmark and tool, each cell contains tripplets of mean, median and number of timeouts. Timeouts include errors and out of memories.\n",
    "\n",
    "If you wish to plot means and timeouts only, change the following line:\n",
    "```python\n",
    "cols = 2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aac64f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols = 3\n",
    "def count_short_stats(series):\n",
    "    \"\"\"For @p series counts either tripple (mean, median, timeouts) or (mean, timeouts)\"\"\"\n",
    "    global cols\n",
    "    vals = [float(v) if is_real(v) else value_to_float(v, 60) for v in series] or [-1]\n",
    "    if cols == 3:\n",
    "        to_len = len([s for s in series if str(s).startswith('TO')])\n",
    "        err_len = len([s for s in series if str(s).startswith('ERR')])\n",
    "        return (\n",
    "            round(np.nanmean(vals), 1), \n",
    "            round(np.nanmedian(vals), 1), \n",
    "            f\"{to_len}\" if err_len == 0 else f\"{to_len}({err_len})\"\n",
    "        )\n",
    "    else:\n",
    "        return (round(np.nanmean(vals), 1), len([s for s in series if str(s).startswith('TO')]))\n",
    "    \n",
    "def is_win(c):\n",
    "    \"\"\"Highlights @p c if it is good value\"\"\"\n",
    "    if is_real(c) and float(c) < 10:\n",
    "        return '{{{0}}}'.format(c)\n",
    "    else:\n",
    "        return str(c)\n",
    "def to_cell(stats, n):\n",
    "    \"\"\"Transforms @p n -th statistic stored in @p stats into a cell\"\"\"\n",
    "    cell = stats.get(n, '-')\n",
    "    if cell == '-' or (np.isnan(cell[0]) and (cell[-1] == 0 or cell[-1] == '0')):\n",
    "        return '\\\\multicolumn{{{0}}}{{c|}}{{-}}'.format(cols)\n",
    "    else:\n",
    "        if cols == 3:\n",
    "            return f\"{is_win(cell[0])} & {is_win(cell[1])} & {is_win(cell[2])}\"\n",
    "        else:\n",
    "            return f\"{is_win(cell[0])} & {is_win(cell[1])}\"\n",
    "        \n",
    "# Groups dataframes by benchmarks, and computs shorts stats for each benchmark and tool.\n",
    "grp_df = overall_df.copy()\n",
    "grp_df['benchmark'] = [get_benchmark_name(b) for b in grp_df['name']]\n",
    "grp_df = grp_df[[c for c in grp_df.columns if c != 'name']]\n",
    "b_param = 'b-param'\n",
    "benchmark_names = sorted(list(set(grp_df['benchmark'])))\n",
    "benchmark_names = [b for b in benchmark_names if b != b_param] + ([b_param] if b_param in benchmark_names else [])\n",
    "headers = ['tool'] + [\"\\\\multicolumn{{{1}}}{{c}}{{{0}}}\".format(b, cols) for b in benchmark_names]\n",
    "data = []\n",
    "\n",
    "grp_items = grp_df.groupby('benchmark').agg(count_short_stats).items()\n",
    "for group in grp_items:\n",
    "    if group[0] == 'bench':\n",
    "        continue\n",
    "    #tool = tool_to_latex[unify_tool(group[0])]\n",
    "    tool = unify_tool(group[0])\n",
    "    stats = group[1].to_dict()\n",
    "    data.append([tool] + [to_cell(stats, n) for n in benchmark_names])\n",
    "data = sorted(data, key=lambda x: x[0])\n",
    "# Creates a splice for BRE benchmarks; note that the indexes are fixed wrt ordering of the benchmarks\n",
    "dheaders = ['tool'] + headers[1:]\n",
    "ddata = [[d[0]] + d[1:] for d in data]\n",
    "with open(os.path.join('figs', 'stats.html'), 'w') as stats_handle:\n",
    "    stats_handle.write(tabulate.tabulate(ddata, headers=dheaders, tablefmt='html'))\n",
    "with open(os.path.join('figs', f'stats{\"-mean-only\" if cols == 2 else \"\"}.tex'), 'w') as stats_handle:\n",
    "    stats_handle.write(\"\\n\".join(tabulate.tabulate(ddata, headers=dheaders, tablefmt='latex_raw').split(\"\\n\")[2:-1]))\n",
    "print(tabulate.tabulate(ddata, headers=dheaders))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8277bfd8",
   "metadata": {},
   "source": [
    "## Lineplot (#line)\n",
    "The following plots cummulative graph of times. For each tool, it prints how long it took to process the benchmarks, if ordered by runtime. Continuous timeouts are omitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb4f56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_generator(series, timeout):\n",
    "    \"\"\"Cumulatively sums the @p series wrt @p timeout\"\"\"\n",
    "    sum = 0\n",
    "    for num in series:\n",
    "        if num >= timeout:\n",
    "            yield None\n",
    "        else:\n",
    "            sum += num\n",
    "            yield sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40753bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BEST_RATE = 0.2\n",
    "\n",
    "item_no = len(plottable_map_per_grp.items())\n",
    "\n",
    "x_dim = item_no // 3 + 1\n",
    "y_dim = min(item_no, 3)\n",
    "fig, ax = plt.subplots(x_dim, y_dim, figsize=(x_dim*5, y_dim*6))\n",
    "plt.subplots_adjust(top = 0.99, bottom=0.01, hspace=0.1, wspace=0.1)\n",
    "bench_list = [\n",
    "    'b-hand-made', 'b-armc-incl', 'b-regex', 'b-smt'                        # BRE Benchmarks\n",
    "]\n",
    "\n",
    "for key, pdf in plottable_map_per_grp.items():\n",
    "    bench, timeout = key.split('#')\n",
    "    timeout = int(timeout)\n",
    "    subgroups = set(list(pdf['benchmark']))\n",
    "    for grp in subgroups:\n",
    "        data = pdf[pdf['benchmark'] == grp]\n",
    "        # Parametric benchmarks will be visualized in other way.\n",
    "        if grp.endswith('-param'):\n",
    "            continue\n",
    "        i = bench_list.index(grp)\n",
    "        sums = defaultdict(list)\n",
    "        grp_name = f\"{grp}\"\n",
    "\n",
    "        # Preprocesses the data to cumulative suns. Filters out those that did not solve\n",
    "        # at least 20% of benchmarks in given groups\n",
    "        for _, row in data.iterrows():\n",
    "            sums[row['tool']].append(row['duration'])\n",
    "        vdata = {}\n",
    "        for k in sorted(sums.keys()):\n",
    "            v = sums[k]\n",
    "            values = list(sum_generator(sorted(v), timeout))\n",
    "            val_len = len(values)\n",
    "            to_len = len([a for a in values if a == None])\n",
    "            if (val_len - to_len) / val_len > BEST_RATE:\n",
    "                vdata[k] = values\n",
    "                \n",
    "        # Plot is in logarithmic scale\n",
    "        g = seaborn.lineplot(\n",
    "            vdata, linewidth=3.5, ax=ax[i // 3, i % 3] if item_no > 1 else ax\n",
    "        )\n",
    "        g.set(yscale=\"symlog\")\n",
    "        g.set_xticklabels(g.get_xticklabels(), rotation=30)\n",
    "        g.set_title(f\"{grp_name}\", x=0.05)    \n",
    "        if i % 3 == 0:\n",
    "            g.set_ylabel(\"time [s]\")\n",
    "        if i // 3 == 2:\n",
    "            g.set_xlabel(\"benchmark\")\n",
    "        seaborn.move_legend(g, \"upper left\", bbox_to_anchor=(0., 1), frameon=True)\n",
    "        i += 1\n",
    "\n",
    "# Saves figures into `figs/` directory\n",
    "save_figure(f\"cactus\")\n",
    "save_figure(f\"cactus\", ext=\".pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1836e65",
   "metadata": {},
   "source": [
    "## Infering Parametric models\n",
    "\n",
    "For two benchmarks `a-param` and `b-param` we created individual models, that models runtime (on `y` axis) based on the value of parameter `k`. Refer to our paper or main site to learn more about the individual families."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2b2986",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['bench', 'k', 'tool', 'duration']\n",
    "def to_models(df):\n",
    "    \"\"\"Computes models out of @p df\"\"\"\n",
    "    data = []\n",
    "    for index, row in df.iterrows():\n",
    "        if 'parametric_ltl' in row['name']:\n",
    "            bench_parts = row['name'].split(os.sep)\n",
    "        else:\n",
    "            bench_parts = os.path.dirname(row['name']).split(os.sep)\n",
    "        bench = os.path.join(*bench_parts[:3])\n",
    "        k = bench_parts[-1]\n",
    "        if is_real(k):\n",
    "            k = int(k)\n",
    "            for col, v in row.items():\n",
    "                if col == 'name':\n",
    "                    continue\n",
    "                tool = unify_tool(col)\n",
    "                if v == 'ERR':\n",
    "                    time = np.NaN\n",
    "                elif is_real(v):\n",
    "                    time = float(v)\n",
    "                elif v.startswith('TO'):\n",
    "                    time = int(v[2:])\n",
    "                else:\n",
    "                    continue\n",
    "                data.append([bench, k, tool, time])\n",
    "    tmp_data = sorted(data, key=lambda x: x[1])\n",
    "    to_map = defaultdict(list)\n",
    "    data = []\n",
    "    for val in tmp_data:\n",
    "        key = f\"{val[0]}:{val[2]}\"\n",
    "        if val[-1] >= 60:\n",
    "            # it's timeout:\n",
    "            if not to_map[key] or to_map[key][-1] < 60:\n",
    "                data.append(val)\n",
    "        else:\n",
    "            data.append(val) \n",
    "        to_map[key].append(val[-1])\n",
    "    models = pandas.DataFrame(data, columns=columns)\n",
    "    return models\n",
    "for key, df in dataframe_map.items():\n",
    "    models = to_models(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9282db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of measured benchmarks to names use in paper\n",
    "bench_to_paper = {\n",
    "    'bool_comb/intersect/longstrings': 'longstrings',\n",
    "    'bool_comb/intersect/expbranching': 'expbranching',\n",
    "    'bool_comb/intersect/exppaths1': \"exppaths1\",\n",
    "    'bool_comb/intersect/exppaths2': \"exppaths2\",\n",
    "    'bool_comb/cox/diff_sat': \"diff_sat\",\n",
    "    'bool_comb/cox/diff_unsat': \"diff_unsat\",\n",
    "    'bool_comb/cox/inter_sat': \"inter_sat\",\n",
    "    'bool_comb/cox/inter_unsat': \"inter_unsat\",\n",
    "    'ltl_afa/created_ltl/LTLf-specific': 'LTLf-specific',\n",
    "    'ltl_afa/parametric_ltl/lift_afas': 'lift_afas',\n",
    "    'ltl_afa/parametric_ltl/counter_afas': 'counter_afas',\n",
    "}\n",
    "\n",
    "\n",
    "f, axs = plt.subplots(4, 3, figsize=(20, 6))\n",
    "plt.subplots_adjust(top = 0.99, bottom=0.01, hspace=0.5, wspace=0.0)\n",
    "i = 0\n",
    "for key in sorted(dataframe_map.keys()):\n",
    "    df = dataframe_map[key]\n",
    "    models = to_models(df)\n",
    "    for bench in sorted(set(models['bench'])):\n",
    "        # The following model is skipped as it is not that interesting\n",
    "        if bench in ('ltl_afa/created_ltl/LTLf-specific',):\n",
    "            continue\n",
    "        data_df = models[models['bench'] == bench]\n",
    "        g = seaborn.pointplot(\n",
    "            data_df, x=\"k\", y=\"duration\", hue=\"tool\", errorbar=None, \n",
    "            ax=axs[i // 3, i % 3], palette=color_map, markers='o',\n",
    "        )\n",
    "        g.legend([],[], frameon=False)\n",
    "        g.set_ylim([0, 60])\n",
    "        g.set_xticklabels(g.get_xticklabels(), rotation=30)\n",
    "        tick_rate = len(g.get_xticklabels()) // 10\n",
    "        if tick_rate != 0:\n",
    "            for index, label in enumerate(g.get_xticklabels()):\n",
    "                if index % tick_rate == 0:\n",
    "                    label.set_visible(True)\n",
    "                else:\n",
    "                    label.set_visible(False)\n",
    "        g.set_title(f\"{bench_to_paper[bench]}\", x=0.05)\n",
    "        if i % 3 != 0:\n",
    "            g.set(yticklabels=[])\n",
    "            g.set(ylabel=None)\n",
    "        seaborn.move_legend(g, \"upper right\", bbox_to_anchor=(1, 1), ncols=2)\n",
    "        # The following are some black magic hacks have the final layout\n",
    "        i += 1\n",
    "        if i == 4 or i == 7:\n",
    "            i += 1\n",
    "# The following are some black magic hacks have the final layout\n",
    "i = 4\n",
    "axs[i // 3, i % 3].set(xlabel=None)\n",
    "axs[i // 3, i % 3].set(yticklabels=[])\n",
    "axs[i // 3, i % 3].set(xticklabels=[])\n",
    "axs[i // 3, i % 3].set(xticks=[])\n",
    "axs[i // 3, i % 3].set(yticks=[])\n",
    "axs[i // 3, i % 3].spines['top'].set_visible(False)\n",
    "axs[i // 3, i % 3].spines['bottom'].set_visible(False)\n",
    "\n",
    "# The following are some black magic hacks to print the label in the middle\n",
    "i = 7\n",
    "axs[i // 3, i % 3].legend(\n",
    "    handles=[\n",
    "        Line2D(\n",
    "            [0], [0], color='w', marker='o', markerfacecolor=color_map[tool], label=f\"{tool}\", \n",
    "            markersize=10,\n",
    "        )\n",
    "        for tool in sorted(color_map.keys())\n",
    "    ], ncols=2, loc='lower center', fontsize='18'\n",
    ")\n",
    "axs[i // 3, i % 3].set(xlabel=None)\n",
    "axs[i // 3, i % 3].set(yticklabels=[])\n",
    "axs[i // 3, i % 3].set(xticklabels=[])\n",
    "axs[i // 3, i % 3].set(xticks=[])\n",
    "axs[i // 3, i % 3].set(yticks=[])\n",
    "axs[i // 3, i % 3].spines['top'].set_visible(False)\n",
    "axs[i // 3, i % 3].spines['bottom'].set_visible(False)\n",
    "save_figure(f'models')\n",
    "save_figure(f'models', ext='.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b36ac4e",
   "metadata": {},
   "source": [
    "## Scatter plot of winners\n",
    "\n",
    "For each benchmark we picked three winners and compared each other in scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd1349a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TO\n",
    "overall_df['bench'] = [get_benchmark_name(n) for n in overall_df['name']]\n",
    "benches = sorted([b for b in sorted(list(set(overall_df['bench']))) if not b.endswith('-param')])\n",
    "\n",
    "winners = {\n",
    "    'a-ltl': ['abc', 'jaltimpact', 'afaminisat-nt'], \n",
    "    'a-ltlf-patterns': ['abc', 'jaltimpact', 'afaminisat-nt'], \n",
    "    'a-ltl-param': ['abc', 'jaltimpact', 'bisim'], \n",
    "    'a-ltl-rand': ['abc', 'bisim', 'jaltimpact'], \n",
    "    'a-sloth': ['abc', 'afaminisat-nt', 'bisim'], -> skipped as it is less interesting\n",
    "    'a-sloth': ['abc', 'afaminisat-nt', 'jaltimpact'],\n",
    "    'a-noodler': ['abc', 'afaminisat-nt', 'jaltimpact'], \n",
    "    'a-ltl-spec': ['afaminisat-nt', 'abc', 'jaltimpact'], \n",
    "    'b-hand-made': ['automata', 'mata-nfa', 'vata'],\n",
    "    'b-armc-incl': ['vata', 'mata-nfa', 'automata'], \n",
    "    'b-param': ['z3', 'afaminisat-nt', 'vata'],\n",
    "    'b-regex': ['automata', 'abc', 'mata-nfa'], \n",
    "    'b-smt': ['afaminisat-nt', 'mata-nfa', 'vata']\n",
    "}\n",
    "indices = [(1, 2), (1, 3), (2, 3)]\n",
    "bench_colors = mpl.colormaps['tab10'].resampled(len(benches)).colors\n",
    "fig = plt.figure(constrained_layout=True, figsize=(4*5, 3*2.5))\n",
    "plt.subplots_adjust(top = 0.99, bottom=0.01, hspace=0.1, wspace=0.3)\n",
    "subfigs = fig.subfigures(3, 3)\n",
    "for i, subfig in enumerate(subfigs.flat):\n",
    "    if i == len(benches):\n",
    "        break\n",
    "    subfig.suptitle(f\"{benches[i]}\")\n",
    "    axs = subfig.subplots(1, 3)\n",
    "    bench = benches[i]\n",
    "    bench_df = df_to_float(overall_df[overall_df['bench'] == bench], 60, allow_timeouts=True)\n",
    "    for ii, ax in enumerate(axs.flat):\n",
    "        lhs, rhs = indices[ii]\n",
    "        lhs, rhs = winners[bench][lhs-1], winners[bench][rhs-1]\n",
    "        ax.grid(True, which='both', linestyle='--')\n",
    "        ax.set_xlabel(f\"{tool_to_output[lhs]}\")\n",
    "        ax.set_ylabel(f\"{tool_to_output[rhs]}\")\n",
    "        ax.scatter(\n",
    "            bench_df[f\"{lhs}-runtime\"], bench_df[f\"{rhs}-runtime\"], marker= 'x', c=[bench_colors[i]]\n",
    "        )\n",
    "        pt = min(max(ax.get_xlim()[1], ax.get_ylim()[1]), 60)\n",
    "        ax.plot([0, pt], [0, pt], linestyle='--', color='red')\n",
    "save_figure(\"scatter\")\n",
    "save_figure(\"scatter\", ext=\".pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecdb836",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Processing experiments complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
